{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers.optimization import AdamW, get_cosine_schedule_with_warmup\n",
    "from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import GPT2LMHeadModel\n",
    "\n",
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q_TKN = \"<usr>\"\n",
    "A_TKN = \"<sys>\"\n",
    "BOS = '</s>'\n",
    "EOS = '</s>'\n",
    "MASK = '<unused0>'\n",
    "PAD = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'GPT2Tokenizer'. \n",
      "The class this function is called from is 'PreTrainedTokenizerFast'.\n",
      "Some weights of GPT2LMHeadModel were not initialized from the model checkpoint at skt/kogpt2-base-v2 and are newly initialized: ['transformer.h.3.attn.forced_bias', 'transformer.h.10.attn.forced_bias', 'transformer.h.2.attn.forced_bias', 'transformer.h.5.attn.forced_bias', 'transformer.h.0.attn.forced_bias', 'transformer.h.9.attn.forced_bias', 'transformer.h.4.attn.forced_bias', 'transformer.h.1.attn.forced_bias', 'transformer.h.11.attn.forced_bias', 'transformer.h.8.attn.forced_bias', 'transformer.h.7.attn.forced_bias', 'transformer.h.6.attn.forced_bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "koGPT2_TOKENIZER = PreTrainedTokenizerFast.from_pretrained(\"skt/kogpt2-base-v2\",\n",
    "            bos_token=BOS, eos_token=EOS, unk_token='<unk>',\n",
    "            pad_token=PAD, mask_token=MASK) \n",
    "model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Chatbot_Data = pd.read_csv(\"../../Data_preprocessing/custom_chatbotdataset(Training).csv\")\n",
    "Chatbot_Data_validation = pd.read_csv(\"../../Data_preprocessing/custom_chatbotdataset(Validation).csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(145954, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chatbot_Data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>Q</th>\n",
       "      <th>A</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9</td>\n",
       "      <td>일은 왜 해도 해도 끝이 없을까? 화가 난다.</td>\n",
       "      <td>많이 힘드시겠어요. 주위에 의논할 상대가 있나요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9</td>\n",
       "      <td>이번 달에 또 급여가 깎였어! 물가는 오르는데 월급만 자꾸 깎이니까 너무 화가 나.</td>\n",
       "      <td>급여가 줄어 속상하시겠어요. 월급이 줄어든 것을 어떻게 보완하실 건가요?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9</td>\n",
       "      <td>회사에 신입이 들어왔는데 말투가 거슬려. 그런 애를 매일 봐야 한다고 생각하니까 스...</td>\n",
       "      <td>회사 동료 때문에 스트레스를 많이 받는 것 같아요. 문제 해결을 위해 어떤 노력을 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>직장에서 막내라는 이유로 나에게만 온갖 심부름을 시켜. 일도 많은 데 정말 분하고 ...</td>\n",
       "      <td>관련 없는 심부름을 모두 하게 되어서 노여우시군요. 어떤 것이 상황을 나아질 수 있...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>얼마 전 입사한 신입사원이 나를 무시하는 것 같아서 너무 화가 나.</td>\n",
       "      <td>무시하는 것 같은 태도에 화가 나셨군요. 상대방의 어떤 행동이 그런 감정을 유발하는...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                                  Q  \\\n",
       "0      9                          일은 왜 해도 해도 끝이 없을까? 화가 난다.   \n",
       "1      9     이번 달에 또 급여가 깎였어! 물가는 오르는데 월급만 자꾸 깎이니까 너무 화가 나.   \n",
       "2      9  회사에 신입이 들어왔는데 말투가 거슬려. 그런 애를 매일 봐야 한다고 생각하니까 스...   \n",
       "3      9  직장에서 막내라는 이유로 나에게만 온갖 심부름을 시켜. 일도 많은 데 정말 분하고 ...   \n",
       "4      9              얼마 전 입사한 신입사원이 나를 무시하는 것 같아서 너무 화가 나.   \n",
       "\n",
       "                                                   A  \n",
       "0                        많이 힘드시겠어요. 주위에 의논할 상대가 있나요?  \n",
       "1           급여가 줄어 속상하시겠어요. 월급이 줄어든 것을 어떻게 보완하실 건가요?  \n",
       "2  회사 동료 때문에 스트레스를 많이 받는 것 같아요. 문제 해결을 위해 어떤 노력을 ...  \n",
       "3  관련 없는 심부름을 모두 하게 되어서 노여우시군요. 어떤 것이 상황을 나아질 수 있...  \n",
       "4  무시하는 것 같은 태도에 화가 나셨군요. 상대방의 어떤 행동이 그런 감정을 유발하는...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chatbot_Data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Chatbot_Data['label'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatbotDataset(Dataset):\n",
    "    def __init__(self, chats, max_len=40):  # 데이터셋의 전처리를 해주는 부분\n",
    "        self._data = chats\n",
    "        self.max_len = max_len\n",
    "        self.q_token = Q_TKN\n",
    "        self.a_token = A_TKN\n",
    "        self.eos = EOS\n",
    "        self.mask = MASK\n",
    "        self.tokenizer = koGPT2_TOKENIZER\n",
    "\n",
    "    def __len__(self):  # chatbotdata 의 길이를 리턴한다.\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):  # 로드한 챗봇 데이터를 차례차례 DataLoader로 넘겨주는 메서드\n",
    "        turn = self._data.iloc[idx]\n",
    "        \n",
    "        q = turn[\"Q\"]  # 질문을 가져온다.\n",
    "        q = re.sub(r\"([?.!,])\", r\" \", q)  # 구둣점들을 제거한다.\n",
    "\n",
    "        a = turn[\"A\"]  # 답변을 가져온다.\n",
    "        a = re.sub(r\"([?.!,])\", r\" \", a)  # 구둣점들을 제거한다.\n",
    "\n",
    "        q_toked = self.tokenizer.tokenize(self.q_token + q)\n",
    "        q_len = len(q_toked)\n",
    "\n",
    "        a_toked = self.tokenizer.tokenize(self.a_token + a + self.eos)\n",
    "        a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이가 최대길이보다 크면\n",
    "        if q_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        #질문의 길이 + 답변의 길이가 최대길이보다 크면\n",
    "        if q_len + a_len > self.max_len:\n",
    "            a_len = self.max_len - q_len        #답변의 길이를 최대길이 - 질문길이\n",
    "            if a_len <= 0:       #질문의 길이가 너무 길어 질문만으로 최대 길이를 초과 한다면\n",
    "                q_toked = q_toked[-(int(self.max_len / 2)) :]   #질문길이를 최대길이의 반으로 \n",
    "                q_len = len(q_toked)\n",
    "                a_len = self.max_len - q_len              #답변의 길이를 최대길이 - 질문길이\n",
    "            a_toked = a_toked[:a_len]\n",
    "            a_len = len(a_toked)\n",
    "\n",
    "        # 답변 labels = [mask, mask, ...., mask, ..., <bos>,..답변.. <eos>, <pad>....]\n",
    "        labels = [self.mask,] * q_len + a_toked[1:]\n",
    "\n",
    "        # mask = 질문길이 0 + 답변길이 1 + 나머지 0\n",
    "        mask = [0] * q_len + [1] * a_len + [0] * (self.max_len - q_len - a_len)\n",
    "        # 답변 labels을 index 로 만든다.\n",
    "        labels_ids = self.tokenizer.convert_tokens_to_ids(labels)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(labels_ids) < self.max_len:\n",
    "            labels_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        # 질문 + 답변을 index 로 만든다.    \n",
    "        token_ids = self.tokenizer.convert_tokens_to_ids(q_toked + a_toked)\n",
    "        # 최대길이만큼 PADDING\n",
    "        while len(token_ids) < self.max_len:\n",
    "            token_ids += [self.tokenizer.pad_token_id]\n",
    "\n",
    "        #질문+답변, 마스크, 답변\n",
    "        return (np.array(token_ids), np.array(mask), np.array(labels_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = ChatbotDataset(Chatbot_Data, max_len=40)\n",
    "train_dataloader = DataLoader(train_set, batch_size=128, num_workers=4, shuffle=False)\n",
    "\n",
    "valid_set = ChatbotDataset(Chatbot_Data_validation, max_len=40)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=128, num_workers=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT2LMHeadModel(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(51200, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D()\n",
       "          (c_proj): Conv1D()\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=51200, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(device)\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 3e-5\n",
    "criterion = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epoch = 30\n",
    "Sneg = -1e18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1141 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape = torch.Size([128, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/1141 [00:01<22:06,  1.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape = torch.Size([128, 40])\n",
      "token_ids shape = torch.Size([128, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 4/1141 [00:01<06:00,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape = torch.Size([128, 40])\n",
      "token_ids shape = torch.Size([128, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/1141 [00:01<04:28,  4.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape = torch.Size([128, 40])\n",
      "token_ids shape = torch.Size([128, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1141 [00:02<03:45,  5.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape = torch.Size([128, 40])\n",
      "token_ids shape = torch.Size([128, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1141 [00:02<03:27,  5.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape = torch.Size([128, 40])\n",
      "token_ids shape = torch.Size([128, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 12/1141 [00:02<03:17,  5.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token_ids shape = torch.Size([128, 40])\n",
      "token_ids shape = torch.Size([128, 40])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 14/1141 [00:03<03:13,  5.83it/s]"
     ]
    }
   ],
   "source": [
    "print (\"start\")\n",
    "for epoch in range(epoch):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for samples in tqdm(train_dataloader): # train\n",
    "        optimizer.zero_grad()\n",
    "        token_ids, mask, label = samples\n",
    "        \n",
    "        token_ids = token_ids.to(device)\n",
    "        mask = mask.to(device)\n",
    "        label = label.to(device)\n",
    "        print('token_ids shape =', token_ids.shape)\n",
    "        out = model(token_ids)\n",
    "        \n",
    "        out = out.logits      #Returns a new tensor with the logit of the elements of input\n",
    "        mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
    "        mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
    "        loss = criterion(mask_out.transpose(2, 1), label)\n",
    "        # 평균 loss 만들기 avg_loss[0] / avg_loss[1] <- loss 정규화\n",
    "        avg_loss = loss.sum() / mask.sum()\n",
    "        avg_loss.backward()\n",
    "        # 학습 끝\n",
    "        optimizer.step()\n",
    "        train_loss += avg_loss\n",
    "    train_loss = (train_loss / len(train_dataloader))\n",
    "    print('Epoch = {}, train loss = {}'.format((epoch+1), train_loss))\n",
    "        \n",
    "    with torch.no_grad(): # validation\n",
    "        model.eval()\n",
    "        for samples in tqdm(valid_dataloader):\n",
    "            token_ids, mask, label = samples\n",
    "            \n",
    "            token_ids = token_ids.to(device)\n",
    "            mask = mask.to(device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            out = model(token_ids)\n",
    "            print('token_ids shape =', token_ids.shape)\n",
    "            out = out.logits\n",
    "            mask_3d = mask.unsqueeze(dim=2).repeat_interleave(repeats=out.shape[2], dim=2)\n",
    "            mask_out = torch.where(mask_3d == 1, out, Sneg * torch.ones_like(out))\n",
    "            loss = criterion(mask_out.transpose(2, 1), label)\n",
    "            \n",
    "            avg_loss = loss.sum() / mask.sum()\n",
    "\n",
    "            valid_loss += avg_loss\n",
    "        valid_loss = (valid_loss / len(valid_dataloader))\n",
    "        print('Epoch = {}, validation loss = {}'.format((epoch+1), valid_loss))\n",
    "            \n",
    "print (\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save({\n",
    "                'epoch': epoch+1,\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "            }, 'model_EQ2A_Custom_Data_noEmotion_{}.pth'.format(epoch+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 챗봇 테스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     while 1:\n",
    "#         q = input(\"user > \").strip()\n",
    "#         if q == \"quit\":\n",
    "#             break\n",
    "#         a = \"\"\n",
    "#         while 1:\n",
    "#             input_ids = torch.LongTensor(koGPT2_TOKENIZER.encode(Q_TKN + q + SENT + '0' + A_TKN + a)).unsqueeze(dim=0)\n",
    "#             pred = model(input_ids.to(device))\n",
    "#             pred = pred.logits\n",
    "#             gen = koGPT2_TOKENIZER.convert_ids_to_tokens(torch.argmax(pred, dim=-1).squeeze().cpu().numpy().tolist())[-1]\n",
    "#             if gen == EOS:\n",
    "#                 break\n",
    "#             a += gen.replace(\"▁\", \" \")\n",
    "#         print(\"Chatbot > {}\".format(a.strip()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
